{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4tvL4ngnEDS"
      },
      "source": [
        "##PotterGPT (Kind of)\n",
        "\n",
        "More like PotterGPRNNs\n",
        "\n",
        "Use textual data from the book The Philosopher's Stone to train a generative RNN which can complete sentences for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63SP5_sxhWtW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFbEE5PAGOoC"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf2pLFlAhWQ0",
        "outputId": "addf3c81-cf96-499f-c9de-caa3dfba5bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'whiteboard'...\n",
            "remote: Enumerating objects: 173, done.\u001b[K\n",
            "remote: Total 173 (delta 0), reused 0 (delta 0), pack-reused 173\u001b[K\n",
            "Receiving objects: 100% (173/173), 27.92 MiB | 16.68 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Anweshbyte/whiteboard.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCQ_DrbAhZqB"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the text data\n",
        "text = open(\"/content/whiteboard/nbviewer/notebooks/data/harrypotter/Book 1 - The Philosopher's Stone.txt\", 'r').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW1dRtG2ilnl"
      },
      "source": [
        "##Training and Labels\n",
        "We would like to split our input data into Training Data and label.\n",
        "How do we go about it?\n",
        "\n",
        "Say we have a sentence:- I love the Transformers.\n",
        "\n",
        "The training data will look like: \"I love the\"\n",
        "and the corresponding label will look like: \"I love the T\"\n",
        "That is, the model must predict the next letter in the sequence.\n",
        "\n",
        "The next training sample will be shifted to the right by one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVtWd2-Fipy4",
        "outputId": "473eecf3-1e4e-4ea3-a189-220d02c19767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The Transf', 'he Transfo', 'e Transfor', ' Transform', 'Transforme', 'ransformer', 'ansformer ', 'nsformer N', 'sformer No', 'former Nos', 'ormer Nost', 'rmer Nostr', 'mer Nostra', 'er Nostrad', 'r Nostrada', ' Nostradam', 'Nostradamu']\n",
            "['o', 'r', 'm', 'e', 'r', ' ', 'N', 'o', 's', 't', 'r', 'a', 'd', 'a', 'm', 'u', 's']\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"The Transformer Nostradamus\"\n",
        "sequence_length = 10\n",
        "sequences = []\n",
        "next_chars = []\n",
        "for i in range(len(sample_text) - sequence_length):\n",
        "    sequences.append(sample_text[i:i + sequence_length])\n",
        "    next_chars.append(sample_text[i+sequence_length]) #Predict the next character in the sequence\n",
        "\n",
        "print(sequences)\n",
        "print(next_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpfIUD-xhgu2"
      },
      "outputs": [],
      "source": [
        "# Create input-output pairs\n",
        "\n",
        "sequence_length = 50\n",
        "sequences = []\n",
        "next_chars = []\n",
        "for i in range(len(text) - sequence_length):\n",
        "    sequences.append(text[i:i + sequence_length])    # Apply the previous operation on the entire text\n",
        "    next_chars.append(text[i+sequence_length])   # Apply the previous operation on the entire text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI4W1blLhg0i",
        "outputId": "327fd46b-4fbc-42fc-ed24-b661e2b4441e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-4b0450f7087f>:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sequences), sequence_length, num_chars), dtype=np.bool)\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the data\n",
        "chars = np.unique(sorted(text)) # Create a list of unique characters present in the text data\n",
        "char_indices = {char: index for index, char in enumerate(chars)}\n",
        "num_chars = len(chars)\n",
        "\n",
        "x = np.zeros((len(sequences), sequence_length, num_chars), dtype=np.bool)\n",
        "y = np.zeros(shape=(len(sequences),num_chars)) # Initialize a 2D numpy array with dimensions (len(sequences), num_chars). It will be used to store the target data for the neural network. each element in y is initialized to zero.\n",
        "\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i, t, char_indices[char]] = 1  # Set the value at the appropriate position in the x array to 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cK7L9eMhlLk"
      },
      "outputs": [],
      "source": [
        "# Build the RNN model\n",
        "model = Sequential() # define a sequential model\n",
        "\n",
        "model.add(LSTM(128, input_shape=(sequence_length, num_chars))) # Add LSTM layer with 128 features and proper input shape\n",
        "model.add(Dense(num_chars, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUoUASDKhof5"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcauZSpShpB2",
        "outputId": "fdd455a5-0d0a-4cbd-d97e-06d1bd86182a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7413/7413 - 44s - loss: 2.1921 - 44s/epoch - 6ms/step\n",
            "Epoch 2/5\n",
            "7413/7413 - 35s - loss: 1.7404 - 35s/epoch - 5ms/step\n",
            "Epoch 3/5\n",
            "7413/7413 - 34s - loss: 1.5883 - 34s/epoch - 5ms/step\n",
            "Epoch 4/5\n",
            "7413/7413 - 35s - loss: 1.4952 - 35s/epoch - 5ms/step\n",
            "Epoch 5/5\n",
            "7413/7413 - 35s - loss: 1.4324 - 35s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7baa611ff2b0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Train the model\n",
        "epochs = 5 # With the small number of training epochs, the model won't learn to form coherent sentences. The easiest thing you can do to improve the results is to train it for longer (try EPOCHS = 30).\n",
        "batch_size = 64\n",
        "model.fit(x, y, epochs=epochs, batch_size = batch_size, verbose=2) # Using model.fit() train the model for a fixed number of epochs (dataset iterations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTcdPNEnhrI3"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "start_index = random.randint(0, len(text)-sequence_length - 1) # Choose random indices between 0(inclusive) and len(text) - sequence_length -1 (exclusive)\n",
        "seed_sequence = text[start_index:start_index + sequence_length]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUxZawOjlG56"
      },
      "source": [
        "We have trained it for only 1 epoch, so the performance of the model is BAD. Try training it for 10 epochs, and then 50 epochs, and you will see a better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_JfapiChtdP",
        "outputId": "e7e61671-1f26-4e1f-8cd3-0935647f018d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The letter from Dumbledore said that Gryaking lin here out aworting whippars \n",
            "stands out of drive a school was jot over the wly my \n",
            "finch atsents up one for again. \n",
            "\n",
            "“Sove easwaggin.” \n",
            "\n",
            "“Wht’ latelon?” said Harry. \n",
            "\n",
            "The smalloppear dark \n"
          ]
        }
      ],
      "source": [
        "generated_text = \"The letter from Dumbledore said that \" #You can play with this!\n",
        "for i in range(200):\n",
        "    x_pred = np.zeros((1, sequence_length, num_chars))\n",
        "    for t, char in enumerate(seed_sequence):\n",
        "        x_pred[0, t, char_indices[char]] = 1\n",
        "    predicted_probs = model.predict(x_pred, verbose=0)[0]\n",
        "    next_char_index = np.random.choice(range(num_chars), p=predicted_probs)\n",
        "    next_char = chars[next_char_index] # Retrieve the character corresponding to the sampled index from the `chars` list and store it in the `next_char` variable.\n",
        "    generated_text += next_char # Append the `next_char` to the `generated_text`\n",
        "    seed_sequence = seed_sequence[1:] + next_char #  Update the `seed_sequence` by removing the first character and adding the `next_char` at the end. This step prepares the updated `seed_sequence` for the next iteration.\n",
        "\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}